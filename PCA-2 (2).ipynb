{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analyis is basically a statistical procedure to convert a set of observation of possibly correlated variables into a set of values of linearly uncorrelated variables.\n",
    "Each of the principal components is chosen in such a way so that it would describe most of the still available variance and all these principal components are orthogonal to each other. In all principal components first principal component has maximum variance.\n",
    "\n",
    "Uses of PCA:\n",
    "\n",
    "It is used to find inter-relation between variables in the data.\n",
    "It is used to interpret and visualize data.\n",
    "As number of variables are decreasing it makes further analysis simpler.\n",
    "It’s often used to visualize genetic distance and relatedness between populations.\n",
    "These are basically performed on square symmetric matrix. It can be a pure sums of squares and cross products matrix or Covariance matrix or Correlation matrix. A correlation matrix is used if the individual variance differs much.\n",
    "\n",
    "Objectives of PCA:\n",
    "\n",
    "It is basically a non-dependent procedure in which it reduces attribute space from a large number of variables to a smaller number of factors.\n",
    "PCA is basically a dimension reduction process but there is no guarantee that the dimension is interpretable.\n",
    "Main task in this PCA is to select a subset of variables from a larger set, based on which original variables have the highest correlation with the principal amount.\n",
    "Principal Axis Method: PCA basically search a linear combination of variables so that we can extract maximum variance from the variables. Once this process completes it removes it and search for another linear combination which gives an explanation about the maximum proportion of remaining variance which basically leads to orthogonal factors. In this method, we analyze total variance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Eigenvector: It is a non-zero vector that stays parallel after matrix multiplication. Let’s suppose x is eigen vector of dimension r of matrix M with dimension r*r if Mx and x are parallel. Then we need to solve Mx=Ax where both x and A are unknown to get eigen vector and eigen values.\n",
    "Under Eigen-Vectors we can say that Principal components show both common and unique variance of the variable. Basically, it is variance focused approach seeking to reproduce total variance and correlation with all components. The principal components are basically the linear combinations of the original variables weighted by their contribution to explain the variance in a particular orthogonal dimension.\n",
    "\n",
    "Eigen Values: It is basically known as characteristic roots. It basically measures the variance in all variables which is accounted for by that factor. The ratio of eigenvalues is the ratio of explanatory importance of the factors with respect to the variables. If the factor is low then it is contributing less in explanation of variables. In simple words, it measures the amount of variance in the total given database accounted by the factor. We can calculate the factor’s eigen value as the sum of its squared factor loading for all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing or loading the dataset \n",
    "dataset = pd.read_csv('e:/Downloads/Wine.csv') \n",
    "\n",
    "# distributing the dataset into two components X and Y \n",
    "X = dataset.iloc[:, 0:13].values \n",
    "y = dataset.iloc[:, 13].values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the X and Y into the \n",
    "# Training set and Testing set \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n",
    "                                                    random_state = 0) \n",
    "#xtrain=X_train\n",
    "#xtest=X_test\n",
    "#ytrain=y_train\n",
    "#ytest=y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Feature Scaling\n",
    "\n",
    "Doing the pre-processing part on training and testing set such as fitting the Standard scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing preprocessing part \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler() \n",
    "\n",
    "X_train = sc.fit_transform(X_train) \n",
    "X_test = sc.transform(X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36884109, 0.19318394])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying PCA function on training \n",
    "# and testing set of X component \n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pca = PCA(n_components = 2) \n",
    "\n",
    "X_train = pca.fit_transform(X_train) \n",
    "X_test = pca.transform(X_test) \n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_ \n",
    "explained_variance\n",
    "\n",
    "#print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Logistic Regression To the training set \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "classifier = LogisticRegression(random_state = 0) \n",
    "classifier.fit(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set result using \n",
    "# predict function under LogisticRegression \n",
    "y_pred = classifier.predict(X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  1  0]\n",
      " [ 1 14  1]\n",
      " [ 0  0  6]]\n",
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# making confusion matrix between \n",
    "# test set of Y and predicted value. \n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "print(cm)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the training set \n",
    "# result through scatter plot \n",
    "from matplotlib.colors import ListedColormap \n",
    "\n",
    "X_set, y_set = X_train, y_train \n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, \n",
    "\t\t\t\t\tstop = X_set[:, 0].max() + 1, step = 0.01), \n",
    "\t\t\t\t\tnp.arange(start = X_set[:, 1].min() - 1, \n",
    "\t\t\t\t\tstop = X_set[:, 1].max() + 1, step = 0.01)) \n",
    "\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), \n",
    "\t\t\tX2.ravel()]).T).reshape(X1.shape), alpha = 0.75, \n",
    "\t\t\tcmap = ListedColormap(('yellow', 'white', 'aquamarine'))) \n",
    "\n",
    "plt.xlim(X1.min(), X1.max()) \n",
    "plt.ylim(X2.min(), X2.max()) \n",
    "\n",
    "for i, j in enumerate(np.unique(y_set)): \n",
    "\tplt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], \n",
    "\t\t\t\tc = ListedColormap(('red', 'green', 'blue'))(i), label = j) \n",
    "\n",
    "plt.title('Logistic Regression (Training set)') \n",
    "plt.xlabel('PC1') # for Xlabel \n",
    "plt.ylabel('PC2') # for Ylabel \n",
    "plt.legend() # to show legend \n",
    "\n",
    "# show scatter plot \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Test set results through scatter plot \n",
    "from matplotlib.colors import ListedColormap \n",
    "\n",
    "X_set, y_set = X_test, y_test \n",
    "\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, \n",
    "\t\t\t\t\tstop = X_set[:, 0].max() + 1, step = 0.01), \n",
    "\t\t\t\t\tnp.arange(start = X_set[:, 1].min() - 1, \n",
    "\t\t\t\t\tstop = X_set[:, 1].max() + 1, step = 0.01)) \n",
    "\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), \n",
    "\t\t\tX2.ravel()]).T).reshape(X1.shape), alpha = 0.75, \n",
    "\t\t\tcmap = ListedColormap(('yellow', 'white', 'aquamarine'))) \n",
    "\n",
    "plt.xlim(X1.min(), X1.max()) \n",
    "plt.ylim(X2.min(), X2.max()) \n",
    "\n",
    "for i, j in enumerate(np.unique(y_set)): \n",
    "\tplt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], \n",
    "\t\t\t\tc = ListedColormap(('red', 'green', 'blue'))(i), label = j) \n",
    "\n",
    "# title for scatter plot \n",
    "plt.title('Logistic Regression (Test set)') \n",
    "plt.xlabel('PC1') # for Xlabel \n",
    "plt.ylabel('PC2') # for Ylabel \n",
    "plt.legend() \n",
    "\n",
    "# show scatter plot \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
